/**
 * Test client-side batching and metadata propagation
 */

import { SupabaseManager } from './supabase.js';

console.log('ğŸ§ª Test: Client-Side Batching & Metadata\n');
console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

// Mock environment variables for GitHub Actions context
process.env.GITHUB_WORKFLOW = 'AISIS Schedule Scrape';
process.env.GITHUB_RUN_ID = '12345678';
process.env.GITHUB_SERVER_URL = 'https://github.com';
process.env.GITHUB_REPOSITORY = 'CelestialBrain/aisis-scraper';
process.env.GITHUB_SHA = 'abc123def456';
process.env.GITHUB_EVENT_NAME = 'schedule';
process.env.SUPABASE_URL = 'https://test-project.supabase.co';

console.log('âœ… Set up mock GitHub Actions environment variables\n');

// Capture requests by overriding the sendRequest method
let capturedRequests = [];

// Create a custom SupabaseManager for testing
class TestSupabaseManager extends SupabaseManager {
  async sendRequest(dataType, records, termCode = null, department = null, programCode = null) {
    // Build metadata just like the parent class
    const metadata = {};
    
    if (termCode) metadata.term_code = termCode;
    if (department) metadata.department = department;
    if (programCode) metadata.program_code = programCode;
    
    if (process.env.GITHUB_WORKFLOW) {
      metadata.workflow_name = process.env.GITHUB_WORKFLOW;
    }
    if (process.env.GITHUB_RUN_ID) {
      metadata.run_id = process.env.GITHUB_RUN_ID;
    }
    if (process.env.GITHUB_SERVER_URL && process.env.GITHUB_REPOSITORY && process.env.GITHUB_RUN_ID) {
      metadata.run_url = `${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}`;
    }
    if (process.env.GITHUB_REPOSITORY) {
      metadata.repository = process.env.GITHUB_REPOSITORY;
    }
    if (process.env.GITHUB_SHA) {
      metadata.commit_sha = process.env.GITHUB_SHA;
    }
    
    if (process.env.GITHUB_EVENT_NAME === 'schedule') {
      metadata.trigger = 'schedule';
    } else if (process.env.GITHUB_EVENT_NAME === 'workflow_dispatch') {
      metadata.trigger = 'manual';
    } else if (process.env.GITHUB_ACTIONS) {
      metadata.trigger = 'github-actions';
    } else {
      metadata.trigger = 'manual';
    }

    const payload = {
      data_type: dataType,
      records: records,
      metadata: metadata
    };

    // Capture the request
    capturedRequests.push({
      url: this.url,
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.ingestToken}`
      },
      payload
    });

    // Simulate success
    console.log(`   ğŸ“Š Edge function response: ${records.length}/${records.length} records upserted`);
    return true;
  }
}

// Create a large dataset to test batching
console.log('ğŸ“¦ Creating test dataset...');
const largeDataset = [];
for (let i = 0; i < 1250; i++) {
  largeDataset.push({
    term_code: '2025-1',
    subject_code: `TEST ${String(i).padStart(3, '0')}`,
    section: 'A',
    department: 'TEST',
    course_title: `Test Course ${i}`,
    units: 3,
    time_pattern: 'MWF 0900-1030',
    room: 'TEST-101',
    instructor: 'Test Instructor',
    language: 'ENG',
    level: 'U',
    remarks: '-',
    max_capacity: 30,
    start_time: '09:00:00',
    end_time: '10:30:00',
    days_of_week: '[1,3,5]',
    delivery_mode: null
  });
}
console.log(`   âœ… Created ${largeDataset.length} mock schedule records\n`);

// Test the SupabaseManager with batching
console.log('ğŸ§ª Testing SupabaseManager batching logic...\n');

const supabase = new TestSupabaseManager('test-token-12345');

console.log(`ğŸ“Š Supabase URL: ${supabase.url}\n`);

// Test syncing with batching
await supabase.syncToSupabase('schedules', largeDataset, '2025-1', 'ALL');

console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
console.log('ğŸ“‹ Verification Results:\n');

// Verify batching
const expectedBatches = Math.ceil(largeDataset.length / 500);
console.log(`âœ“ Expected batches: ${expectedBatches}`);
console.log(`âœ“ Actual requests sent: ${capturedRequests.length}`);
console.log(`âœ“ Batching ${capturedRequests.length === expectedBatches ? 'âœ… CORRECT' : 'âŒ FAILED'}\n`);

// Verify request structure
console.log('ğŸ” Verifying request structure...');
const firstRequest = capturedRequests[0];

console.log(`\nğŸ“¦ First Request Structure:`);
console.log(`   URL: ${firstRequest.url}`);
console.log(`   Authorization header: ${firstRequest.headers.Authorization ? 'âœ… Present' : 'âŒ Missing'}`);
console.log(`   Content-Type: ${firstRequest.headers['Content-Type']}`);

console.log(`\nğŸ“„ Payload Structure:`);
console.log(`   data_type: ${firstRequest.payload.data_type}`);
console.log(`   records.length: ${firstRequest.payload.records.length}`);
console.log(`   metadata keys: ${Object.keys(firstRequest.payload.metadata).join(', ')}`);

// Verify metadata fields
const metadata = firstRequest.payload.metadata;
console.log(`\nğŸ”– Metadata Fields:`);
console.log(`   âœ“ term_code: ${metadata.term_code || 'missing'}`);
console.log(`   âœ“ department: ${metadata.department || 'missing'}`);
console.log(`   âœ“ workflow_name: ${metadata.workflow_name || 'missing'}`);
console.log(`   âœ“ run_id: ${metadata.run_id || 'missing'}`);
console.log(`   âœ“ run_url: ${metadata.run_url || 'missing'}`);
console.log(`   âœ“ repository: ${metadata.repository || 'missing'}`);
console.log(`   âœ“ commit_sha: ${metadata.commit_sha || 'missing'}`);
console.log(`   âœ“ trigger: ${metadata.trigger || 'missing'}`);

// Check all required fields are present
const requiredMetadataFields = [
  'term_code', 'department', 'workflow_name', 'run_id', 
  'run_url', 'repository', 'commit_sha', 'trigger'
];
const missingFields = requiredMetadataFields.filter(field => !metadata[field]);

console.log(`\nğŸ“Š Metadata Validation:`);
if (missingFields.length === 0) {
  console.log(`   âœ… All required metadata fields present`);
} else {
  console.log(`   âŒ Missing fields: ${missingFields.join(', ')}`);
}

// Verify batch sizes
console.log(`\nğŸ“ Batch Sizes:`);
capturedRequests.forEach((req, index) => {
  const batchNum = index + 1;
  const recordCount = req.payload.records.length;
  const expected = index < capturedRequests.length - 1 ? 500 : largeDataset.length % 500 || 500;
  console.log(`   Batch ${batchNum}: ${recordCount} records ${recordCount === expected ? 'âœ…' : 'âŒ'}`);
});

// Verify total records
const totalRecordsSent = capturedRequests.reduce((sum, req) => sum + req.payload.records.length, 0);
console.log(`\nğŸ“Š Total Records:`);
console.log(`   Expected: ${largeDataset.length}`);
console.log(`   Sent: ${totalRecordsSent}`);
console.log(`   Match: ${totalRecordsSent === largeDataset.length ? 'âœ…' : 'âŒ'}`);

// Final summary
console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
if (
  capturedRequests.length === expectedBatches &&
  missingFields.length === 0 &&
  totalRecordsSent === largeDataset.length &&
  firstRequest.headers.Authorization === 'Bearer test-token-12345'
) {
  console.log('âœ… ALL TESTS PASSED!\n');
  console.log('âœ“ Client-side batching works correctly (500 records per batch)');
  console.log('âœ“ All metadata fields are included');
  console.log('âœ“ Authorization header is set correctly');
  console.log('âœ“ All records are sent without loss');
  process.exit(0);
} else {
  console.log('âŒ SOME TESTS FAILED\n');
  process.exit(1);
}
