name: AISIS Schedule Scrape

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Download previous baselines
        # See README.md "Baseline Tracking and Regression Detection" section
        # Downloads baseline-{term}.json files from previous runs for comparison
        uses: actions/download-artifact@v4
        with:
          name: baselines
          path: logs/baselines/
        continue-on-error: true  # Don't fail if no previous baseline exists

      - name: Run scraper
        env:
          AISIS_USERNAME: ${{ secrets.AISIS_USERNAME }}
          AISIS_PASSWORD: ${{ secrets.AISIS_PASSWORD }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          DATA_INGEST_TOKEN: ${{ secrets.DATA_INGEST_TOKEN }}
          GOOGLE_SERVICE_ACCOUNT: ${{ secrets.GOOGLE_SERVICE_ACCOUNT }}
          SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }}
          # Optional: Override term to skip auto-detection (speeds up startup)
          # AISIS_TERM: '2025-1'
          # Optional: Performance tuning (default: 2000)
          # SUPABASE_CLIENT_BATCH_SIZE: '2000'
          # Optional: Regression detection (defaults: 5.0, true)
          # BASELINE_DROP_THRESHOLD: '5.0'
          # BASELINE_WARN_ONLY: 'true'
        run: npm start
      
      - name: Upload baselines for next run
        # See README.md "Baseline Tracking and Regression Detection" section
        # Saves baseline-{term}.json files for comparison in future runs
        uses: actions/upload-artifact@v4
        if: always()  # Upload even if scraper fails
        with:
          name: baselines
          path: logs/baselines/
          retention-days: 90  # Keep baselines for 90 days
